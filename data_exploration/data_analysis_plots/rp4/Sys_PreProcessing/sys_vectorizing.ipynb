{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f644965bf2315af6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Introduction\n",
    "Use vectorizers to read in data. See here: https://stackoverflow.com/questions/31784011/scikit-learn-fitting-data-into-chunks-vs-fitting-it-all-at-once\n",
    "\n",
    "We cannot fit the data all at the same time on the vectorizer, as it takes too much memory. Luckily, this is not needed. We first iterate over chunks of text data and build up the vocabulary of the corpus. Then we can use it to fit the CountVectorizer efficiently.\n",
    "\n",
    "Then we can go over the chunks of text data again and transform them with the CountVectorizer into vectors. We can easily store all vectors of the complete data in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cc8f3206c0bea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:57:55.993760Z",
     "start_time": "2024-05-06T08:57:55.776876Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4b8ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max CPUs to use\n",
    "max_workers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562b185204e98736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:57:55.997422Z",
     "start_time": "2024-05-06T08:57:55.994909Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = Path(\n",
    "    '/media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/')\n",
    "assert data_path.exists()\n",
    "\n",
    "csv_files = [csv_file for csv_file in data_path.glob(\"**/*.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8fb89143adf328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:05:10.360480Z",
     "start_time": "2024-05-06T09:05:10.354413Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    # Using default pattern from CountVectorizer\n",
    "    token_pattern = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "    return [t for t in token_pattern.findall(doc)]\n",
    "\n",
    "\n",
    "def yield_dataframe_file():\n",
    "    # for csv_file in tqdm(csv_files, desc=\"Reading dataframes from csvs\", unit=\"files\"):\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Reading {csv_file}\")\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        yield df\n",
    "\n",
    "def process_df(df):\n",
    "    documents = df[\"system_calls\"].to_numpy()\n",
    "    vocab_df = set()\n",
    "    for doc in documents:\n",
    "        vocabs = set(tokenizer(str(doc)))\n",
    "        vocab_df.update(vocabs)\n",
    "    \n",
    "    print(f\"Processed {len(documents)} documents. Returning {len(vocab_df)} vocabs\")\n",
    "    return vocab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415ad68",
   "metadata": {},
   "source": [
    "### Creating the Vocab for The CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567d6059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab: ('accept', 'accept4', 'access', 'add_key', 'adjtimex', 'bind', 'brk', 'capget', 'capset', 'chdir', 'chmod', 'chown32', 'clock_gettime', 'clone', 'close', 'connect', 'dup', 'dup2', 'dup3', 'epoll_create1', 'epoll_ctl', 'epoll_wait', 'eventfd2', 'execve', 'exit', 'exit_group', 'faccessat', 'fadvise64_64', 'fallocate', 'fchdir', 'fchmod', 'fchmodat', 'fchown32', 'fcntl64', 'fgetxattr', 'flistxattr', 'flock', 'fsetxattr', 'fstat64', 'fstatat64', 'fstatfs64', 'fsync', 'ftruncate64', 'futex', 'getcwd', 'getdents64', 'getegid32', 'geteuid32', 'getgid32', 'getgroups32', 'getpeername', 'getpgid', 'getpgrp', 'getpid', 'getppid', 'getpriority', 'getrandom', 'getresgid32', 'getresuid32', 'getsid', 'getsockname', 'getsockopt', 'gettid', 'gettimeofday', 'getuid32', 'getxattr', 'inotify_add_watch', 'ioctl', 'ioprio_get', 'ioprio_set', 'kcmp', 'keyctl', 'kill', 'lchown32', 'lgetxattr', 'llseek', 'lseek', 'lstat64', 'madvise', 'mkdir', 'mknod', 'mmap2', 'mount', 'mprotect', 'mremap', 'msync', 'munmap', 'name_to_handle_at', 'nan', 'newselect', 'open', 'openat', 'pause', 'perf_event_open', 'pipe', 'pipe2', 'poll', 'prctl', 'pread64', 'prlimit64', 'read', 'readlink', 'readlinkat', 'recv', 'recvfrom', 'recvmsg', 'rename', 'rmdir', 'rt_sigaction', 'rt_sigprocmask', 'rt_sigreturn', 'sched_getaffinity', 'sched_getparam', 'sched_getscheduler', 'sched_yield', 'seccomp', 'semop', 'send', 'sendmmsg', 'sendmsg', 'sendto', 'set_robust_list', 'set_tid_address', 'setgid32', 'setgroups32', 'setitimer', 'setpgid', 'setpriority', 'setregid32', 'setresgid32', 'setresuid32', 'setreuid32', 'setsid', 'setsockopt', 'setuid32', 'setxattr', 'shutdown', 'sigaltstack', 'sigreturn', 'socket', 'socketpair', 'stat64', 'statfs', 'statfs64', 'symlink', 'sysinfo', 'tgkill', 'timerfd_create', 'timerfd_settime', 'ugetrlimit', 'umask', 'umount2', 'uname', 'unlink', 'unlinkat', 'utimensat', 'wait4', 'waitid', 'write', 'writev')\n"
     ]
    }
   ],
   "source": [
    "vocab_file = data_path / \"vocabulary.pkl\"\n",
    "\n",
    "if vocab_file.exists():\n",
    "    with open(str(vocab_file), 'rb') as f:\n",
    "        vocabulary = pickle.load(f)\n",
    "    print(\"Loaded vocab:\", vocabulary)\n",
    "else:\n",
    "    vocab_set = set()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "        futures = [executor.submit(process_df, df) for df in yield_dataframe_file()]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            vocab_set.update(future.result())\n",
    "\n",
    "    vocabulary = tuple(sorted(vocab_set))\n",
    "    with open(str(vocab_file), 'wb') as f:\n",
    "        pickle.dump(vocabulary, f)\n",
    "    \n",
    "    print(\"Saved vocab:\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12fe06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_file = data_path / \"vocabulary.pkl\"\n",
    "\n",
    "# if vocab_file.exists():\n",
    "#     with open(str(vocab_file), 'rb') as f:\n",
    "#         vocabulary = pickle.load(f)\n",
    "#     print(\"Loaded vocab:\", vocabulary)\n",
    "# else:\n",
    "#     vocab_set = set()\n",
    "\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers = max_workers) as executor:\n",
    "#         futures = [executor.submit(process_df, df) for df in yield_dataframe_file()]\n",
    "#         results = [future.result() for future in futures]\n",
    "\n",
    "#     for vocab_df in results:\n",
    "#         vocab_set.update(vocab_df)\n",
    "\n",
    "#     vocab = tuple(sorted(vocab_set))\n",
    "#     with open(str(vocab_file), 'wb') as f:\n",
    "#         pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f165f4",
   "metadata": {},
   "source": [
    "#### Encoding the systemcalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1625f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"1_normal\",\n",
    "    \"2_ransomware\",\n",
    "    \"3_thetick\",\n",
    "    \"4_bashlite\",\n",
    "    \"5_httpbackdoor\",\n",
    "    \"6_beurk\",\n",
    "    \"7_backdoor\",\n",
    "    \"8_bdvl\",\n",
    "    \"9_xmrig\",\n",
    "]\n",
    "label_encoder =  LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de19a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, csv_file):\n",
    "    len_bf = len(df[\"system_calls\"])\n",
    "    df = df[df[\"system_calls\"].notna()]\n",
    "    print(f\"Removed {len_bf} - {len(df)} = {len_bf - len(df)} NaNs from documents in df: \", csv_file)\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"Skipping empty dataframe: \", csv_file)\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    documents = df[\"system_calls\"].to_numpy()\n",
    "    X_docs = vectorizer.transform(documents)\n",
    "    X_docs = X_docs.toarray()\n",
    "\n",
    "    labels = label_encoder.transform(df[\"experiment\"])\n",
    "    cols = np.column_stack((df[\"timestamp\"].to_numpy(), labels))\n",
    "\n",
    "    return X_docs, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b22930f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_backdoor_2h_45.3G.zip_logs.csv\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_backdoor_2h_45.5G.zip_logs.csv\n",
      "Removed 646 - 646 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_backdoor_2h_45.3G.zip_logs.csv\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_bashlite_2h_451.G.zip_logs.csvRemoved 646 - 646 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_backdoor_2h_45.5G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_Bashlite_2h_48.6G.zip_logs.csvRemoved 646 - 646 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_bashlite_2h_451.G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_bdvl_2h_44G.zip_logs.csvRemoved 644 - 644 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_Bashlite_2h_48.6G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_bdvl_2h_45.9G.zip_logs.csvRemoved 575 - 575 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_bdvl_2h_44G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_beurk_2h_43.6G.zip_logs.csvRemoved 578 - 578 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_bdvl_2h_45.9G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_beurk_2h_45.3G.zip_logs.csv\n",
      "Removed 648 - 648 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_beurk_2h_43.6G.zip_logs.csv\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_httpbackdoors_2h_47.8G.zip_logs.csv\n",
      "Removed 650 - 650 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_beurk_2h_45.3G.zip_logs.csv\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_httpbackdoor_2h_46.5G.zip_logs.csvRemoved 647 - 647 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_httpbackdoors_2h_47.8G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_normal_157min_60G.zip_logs.csvRemoved 645 - 645 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_httpbackdoor_2h_46.5G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_normal_83min_32.8G.zip_logs.csvRemoved 846 - 846 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_normal_157min_60G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_ramsomware_90min_19.2G.zip_logs.csv\n",
      "Removed 532 - 428 = 104 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_normal_83min_32.8G.zip_logs.csv\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_ransomware_62min_12.7G.zip_logs.csv\n",
      "Removed 486 - 486 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_ramsomware_90min_19.2G.zip_logs.csv\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_ransomware_90min_18.4G.zip_logs.csvRemoved 341 - 341 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_ransomware_62min_12.7G.zip_logs.csv\n",
      "\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_thetick_2h_43.2G.zip_logs.csv\n",
      "Removed 489 - 489 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_ransomware_90min_18.4G.zip_logs.csv\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_thetick_2h_44.2G.zip_logs.csv\n",
      "Removed 645 - 645 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_thetick_2h_43.2G.zip_logs.csv\n",
      "Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_xmrig_2h_20.3G.zip_logs.csvRemoved 647 - 647 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_thetick_2h_44.2G.zip_logs.csv\n",
      "\n",
      "Removed 612 - 612 = 0 NaNs from documents in df: Reading /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_xmrig_2h_20.4G.zip_logs.csv\n",
      " /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_xmrig_2h_20.3G.zip_logs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4/19 [31:15<1:28:41, 354.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 609 - 609 = 0 NaNs from documents in df:  /media/<User>/DC/IS_Data_Exploration_and_Feature_Engineering_for_an_IoT_Device_Behavior_Fingerprinting_Dataset/sys_system_calls_Heqing_device2/device2_xmrig_2h_20.4G.zip_logs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [32:01<00:00, 101.11s/it] \n"
     ]
    }
   ],
   "source": [
    "X = np.array([[]]).reshape(0, len(vocabulary))\n",
    "Z = np.array([[]]).reshape(0, 2)\n",
    "\n",
    "with tqdm(total=len(csv_files)) as pbar:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_df, df, csv_file) for df, csv_file in zip(yield_dataframe_file(), csv_files)]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            X_docs, cols = future.result()\n",
    "            X = np.concatenate([X, X_docs], axis=0)\n",
    "            Z = np.concatenate([Z, cols], axis=0)\n",
    "            pbar.update(1)\n",
    "\n",
    "assert X.shape[0] == Z.shape[0], f\"X.shape[0] = {X.shape[0]}!= Z.shape[0] = {Z.shape[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "# X = np.array([[]]).reshape(0, len(vocabulary))\n",
    "# Z = np.array([[]]).reshape(0, 2)\n",
    "\n",
    "# with tqdm(total=len(csv_files)) as pbar:\n",
    "#     for i, df in enumerate(yield_dataframe_file()):\n",
    "#         len_bf = len(df[\"system_calls\"])\n",
    "#         df = df[df[\"system_calls\"].notna()]\n",
    "#         print(f\"Removed {len_bf} - {len(df)} = {len_bf - len(df)} NaNs from documents in df: \", csv_files[i])\n",
    "\n",
    "#         documents = df[\"system_calls\"].to_numpy()\n",
    "#         if len(documents) == 0:\n",
    "#             print(\"Skipping empty dataframe: \", csv_files[i])\n",
    "#             continue\n",
    "\n",
    "#         X_docs = vectorizer.transform(documents)\n",
    "#         X = np.concatenate([X, X_docs.toarray()], axis=0)\n",
    "\n",
    "#         labels = label_encoder.transform(df[\"experiment\"])\n",
    "#         cols = np.column_stack((df[\"timestamp\"].to_numpy(), labels))\n",
    "#         Z = np.concatenate([Z, cols], axis=0)\n",
    "\n",
    "#         assert X.shape[0] == Z.shape[0], f\"X.shape[0] = {X.shape[0]} != Z.shape[0] = {Z.shape[0]}\"\n",
    "\n",
    "#         pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a518e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "X_tf = transformer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "069b8b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11428, 160), (11428, 160), (11428, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf.shape, X.shape, Z.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d526aef07a1318",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('vectorizer', CountVectorizer(vocabulary=vocabulary)),\n",
    "#     ('tfidf', TfidfTransformer())\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline to the data and transform it\n",
    "# X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "# X_transformed_dense = X_transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8da5dbb5430aa77a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nan' 'fchown32' 'symlink' 'ioprio_set' 'lseek' 'setregid32' 'keyctl'\n",
      " 'add_key' 'seccomp' 'setpriority']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf_weights = transformer.idf_\n",
    "top_features = np.argsort(idf_weights)[::-1][:10]  # get top 10 features\n",
    "print(feature_names[top_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a0b79db1936a115",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_array = np.column_stack([X_tf, Z])\n",
    "\n",
    "output_file = str(data_path / 'merged_data')\n",
    "np.savez_compressed(output_file, merged_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

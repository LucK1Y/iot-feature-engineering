import os
import sys
import torch
from torch.utils.data import Dataset, DataLoader, TensorDataset
import pandas as pd
from sklearn.model_selection import train_test_split
from pathlib import Path
import lightning as pl
import numpy as np

sys.path.append("/home/ubuntu/fedstellar-development-main")
from fedstellar.learning.pytorch.fedstellardataset import FedstellarDataset


DATASET_IID = "merged_df.csv"
NON_IID_DIRICHLET_ALHPA = 1.0


class SimpleDataset(Dataset):
    def __init__(self, data, targets):
        """
        Args:
            data (torch.Tensor): Tensor containing the features.
            labels (torch.Tensor): Tensor containing the labels.
        """
        self.data = data
        self.targets = targets

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        feature = self.data[idx]
        label = self.targets[idx]
        return feature, label


class MalwaresDataset(FedstellarDataset):
    def __init__(
        self,
        sub_id,
        number_sub,
        iid,
        batch_size=32,
        num_workers=4,
        val_percent=0.2,
    ):
        super().__init__(
            batch_size=batch_size,
            num_workers=num_workers,
            val_percent=val_percent,
            sub_id=sub_id,
            number_sub=number_sub,
            iid=iid,
        )

    def setup(self, stage: str = ""):
        root_dir = Path(f"{sys.path[0]}/data")
        csv_path = root_dir / DATASET_IID

        assert root_dir.exists(), f"Root dir at {root_dir.resolve()} does not exist."
        if not csv_path.exists():
            raise FileNotFoundError(f"Dataset not found at {csv_path}")

        data_frame = pd.read_csv(csv_path)

        if self.iid:
            splits = np.array_split(data_frame, self.number_sub)
            data_frame = splits[self.sub_id]

            # alternatively, we could use the user depedent distribution of data on to nodes
            # self.load_user_depenend_dataset(root_dir)

        labels = torch.tensor(data_frame.pop("label").values, dtype=torch.uint8)
        features = torch.tensor(data_frame.values, dtype=torch.float32)

        X_train, X_test, y_train, y_test = train_test_split(
            features, labels, test_size=self.val_percent, random_state=42
        )

        if self.iid:
            self.train_set = TensorDataset(X_train, y_train)
            self.train_indices_map = list(range(len(self.train_set)))
        else:
            self.train_set = TensorDataset(X_train, y_train)

            dataset = SimpleDataset(data=X_train, targets=y_train)
            partitions_map = self.dirichlet_partition(
                dataset=dataset, alpha=NON_IID_DIRICHLET_ALHPA
            )
            self.train_indices_map = partitions_map[self.sub_id]

        self.test_set = TensorDataset(X_test, y_test)
        self.test_indices_map = list(range(len(self.test_set)))

        print(
            f"MalwaresDataset setup completed {f'(stage: {stage})' if stage else ''}: train_set:{self.train_set.tensors[0].shape}, test_set: {self.test_set.tensors[0].shape}"
        )
        print("Label Distribution Train: ")
        self.check_label_distribution(self.train_set[self.train_indices_map])
        print("Label Distribution Test: ")
        self.check_label_distribution(self.test_set[self.test_indices_map])

    def initialize_dataset(self):
        # Load CIFAR10 train dataset
        if self.train_set is None or self.test_set is None:
            self.setup()

        print(f"Length of train indices map: {len(self.train_indices_map)}")
        print(f"Lenght of test indices map: {len(self.test_indices_map)}")

    def load_user_depenend_dataset(self, root_dir):
        files = np.arange(0, 8)
        rng = np.random.default_rng(22740369)
        rng.shuffle(files)

        if self.number_sub <= 8:
            splits = np.array_split(files, self.number_sub)
            selected_files_ids = splits[self.sub_id]
            dfs = []
            for id in selected_files_ids:
                csv_path = root_dir / "user" / f"{id}.csv"
                if not csv_path.exists():
                    raise FileNotFoundError(f"Dataset not found at {csv_path}")

                print("Reading: ", csv_path)
                dfs.append(pd.read_csv(str(csv_path)))
            data_frame = pd.concat(dfs)
        elif self.number_sub == 16:
            # we store Tuple (id of node; 0/1: first or second half of dataset)
            tuples = [(f, 0) for f in files] + [(f, 1) for f in files]
            rng.shuffle(tuples)

            selected_file_id_and_half = tuples[self.sub_id]
            csv_path = root_dir / "user" / f"{selected_file_id_and_half[0]}.csv"
            print("Reading: ", csv_path, f"and {selected_file_id_and_half[1]} half")
            data_frame = pd.read_csv(str(csv_path))
            splits = train_test_split(data_frame, test_size=0.5, random_state=22740369)
            data_frame = splits[selected_file_id_and_half[1]]
        else:
            raise ValueError("Unsupported number of nodes. Only <= 8 or 16 supported.")
        return data_frame

    def generate_non_iid_map(self, dataset, partition="dirichlet"):
        """
        Create a non-iid map of the dataset.
        """
        pass

    def generate_iid_map(self, dataset):
        """
        Create an iid map of the dataset.
        """
        pass

    def check_label_distribution(self, dataset):
        from collections import Counter

        if type(dataset) is TensorDataset:
            all_labels = [label.item() for label in dataset.tensors[1].numpy()]
        elif type(dataset) is tuple:
            all_labels = [label.item() for label in dataset[1]]
        else:
            raise ValueError("Unsupported dataset type")

        label_counter = Counter(all_labels)

        # Print label distribution
        counts = label_counter.items()
        counts = sorted(counts, key=lambda x: x[0])
        for label, count in counts:
            print(f"Label {label}: {count} samples")


if __name__ == "__main__":
    print("Sanity check csv files")
    malware_path = Path("/home/ubuntu/fedstellar-development-main/fedstellar/data")
    shapes = set()
    for csv in malware_path.glob("*.csv"):
        df = pd.read_csv(str(csv))
        print(df.shape)
        shapes.add(df.shape[1])
    assert len(shapes) == 1
